<style>
.small-code pre code {
  font-size: 1em;
}

.midcenter {
    position: fixed;
    top: 50%;
    left: 50%;
}

.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}

</style>

Intro to extracting and analyzing twitter data
========================================================
author: Rafael Monge
date:   26-11-2015

```{r, echo=FALSE}
opts_chunk$set(out.width='750px', dpi=200)
library(twitteR)
library(RCurl)
library(base64enc)
library(httr)
library(tm)
library(ggplot2)
library(SnowballC)
library(reshape2)
library(sentiment)
library(cluster) 
library(dplyr)
library(tm)
source("GetTweets.R")
source("BuildCorpus.R")
source("keywordChart.R")
source("Clusters.R")
source("Sentiment.R")
```

Agenda
========================================================

- Requirements
- Twitter API
- Extracting Twitter data
- Building a corpus
- Clusters
- Sentiment Analysis


Requirements
========================================================
type: section

Tools used
========================================================

- R
- RStudio
- R libraries:
	- twitteR
	- tm
	- cluster
	- sentiment
	 
Accessing Twitter API
========================================================

1. Create a twitter account
2. Go to https://apps.twitter.com/app/new
3. Fill in the form sample values: Name = TwitterDemoForCOP, description = "test", website = "test.intel.com" and create the app.
4. Get/generate the access tokens.
5. You'll need several values for connecting to twitter.  
	-  Consumer Key (API Key)
	-  Consumer Secret (API Secret)
	-  Access Token
	-  Access Token Secret
	
Accessing Twitter API
========================================================

![Application settings](sourceImages/ApplicationSettings.png)

Accessing Twitter API
========================================================

![Access Tokens](sourceImages/AccessToken.png)

Twitter API
========================================================
type: section

REST API
========================================================
- Get access to tweets, trends, user information etc.
- Can filter tweets by
  - Keyword: "beer -root", "Intel"
  - longitude/latitude
  - dates
  - users
  - language
  - etc.
- https://dev.twitter.com/rest/reference/get/search/tweets

Streaming API
========================================================
- Get a constant stream of  tweets as they are created
- Can filter by:
  - simple keywords
  - location
  - users

Rate limiting
========================================================
- Twitter limits the number of requests 
- 450 requests  / 15 min window for REST
- 1% of tweets for Streaming API
- Can always pay for more tweets :)

Extracting Tweets using R  
========================================================
type: section

using twitteR library

Extracting Tweets using R  
========================================================
class: small-code

```{r, eval=FALSE}
consumerKey <- "consumer!!"
consumerSecret <- "consumerSecret!"
accessToken <- "token!!"
accessSecret <- "secret!!"

setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
intelTweets <- searchTwitter("Intel", n=15000, lang="en")
tweets <- twListToDF(intelTweets)
```


What have we got?
========================================================
class: small-code

```{r, echo =FALSE}
tweets <- GetTweetsFromFile()
```

16 columns:
```{r, echo =FALSE}
names(tweets)
```



What have we got?
========================================================


```{r}
(tweets[2:5,1]) ## get some tweets
```


Posts by User
========================================================

```{r, results='hide', echo=FALSE}
userCount <- tweets %>% group_by(screenName) %>%
  summarise(count = length(id)) %>%
  arrange (desc(count))
userCount <- head(userCount, 10)
ggplot(userCount, aes(screenName, count)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Who are they?
========================================================
![Access Tokens](sourceImages/NeoIntel.png)

Who are they?
========================================================
![Access Tokens](sourceImages/BestDeals.png)


Building a Corpus
========================================================
type: section

using tm library



What's a corpus?
========================================================
class: small-code

```{r, echo = FALSE, results='hide', cache=TRUE}
  # build a corpus
  corpus <- Corpus(VectorSource(tweets$text))
  corpus <-tm_map(corpus, function(x) iconv(x, to='UTF-8', sub='byte'))
  corpus <- tm_map(corpus, tolower) 
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  # "run", "runs" and "running" all become "run").
  corpus <- tm_map(corpus, stemDocument)
  stopWords <- c(stopwords('english'))
  corpus <- tm_map(corpus, removeWords, stopWords)
  corpus <- tm_map(corpus, PlainTextDocument)
```

- Collection of texts and metadata
- Different formats
- Applies common operations regardless of format
- Used to clean tweets
  - Lower case
  - remove punctuation
  - remove stop words
    ```{r, echo = FALSE}
stopWords[1:5]
```  
  - stem words

Transformed Text
========================================================
class: small-code

Raw Tweets
```{r, echo=FALSE}
sampleTramsformedText <- sapply(corpus[2:3], as.character)
names(sampleTramsformedText) <- NULL

(tweets[2:3,1]) ## get some tweets
```

Transformed tweets
```{r, echo=FALSE}
sampleTramsformedText
```

Terms Matrix
========================================================
class: small-code

```{r, cache=TRUE, results='hide', echo=FALSE}
termsMatrix <- TermDocumentMatrix(corpus)
commonTermsMatrix <- removeSparseTerms(termsMatrix, sparse=0.96)
commonTermsDf <- as.data.frame(inspect(commonTermsMatrix))
commonTermsDf <- commonTermsDf[!rownames(commonTermsDf) %in% c('intel'),]
```

```{r}
commonTermsDf[1:5, 1:4]
```

Clustering
========================================================
type: section

Hierarchical Cluster
========================================================
class: small-code


1. Each tweets starts on its own cluster/group
2. Calculate distance between tweets (Tweets with common word counts will be closer)
3. Find the 2 closest clusters and merge them into a single cluster
4. Compute distances (similarities) between the new cluster and each of the old clusters.
5. Repeat steps 3 and 4 until there's only one cluster.


```{r, results='hide', echo=TRUE, echo=FALSE}

commonTermsDf.scale <- scale(commonTermsDf)
d <- dist(commonTermsDf.scale, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
```
  

  
Hierarchical Cluster - plot
========================================================
![Access Tokens](sourceImages/Rplot.png)


Kmeans Cluster
========================================================
1. Define K initial points (centroids).
2. Calculate distance of all data points to the centroids and define which one they are closer to
3. Redefine the centroids as the mean of data points closest to them
4. Repeat until the centroids don't change

```{r,  echo=FALSE}
kfit <- kmeans(d, 5)   
```

kmeans plot
========================================================
class: small-code

```{r, eval=FALSE}
kplot<- clusplot(as.matrix(d), kfit$cluster)  
plot(kplot)
```

![Access Tokens](sourceImages/clusplot.png)

sentiment analysis
========================================================
type: section


```{r, eval=TRUE, echo=FALSE, cache=TRUE}
transformedText<- sapply(corpus, as.character)
sentimentDf <- GetSentimentAnalysisDf(transformedText)
polarityPlot <- GetPolarityPlot(sentimentDf)
emotionPlot <- GetEmotionPlot(sentimentDf)

```

polarity plot
========================================================
```{r, eval=TRUE, echo=FALSE}
plot(polarityPlot)
```

emotion plot 
========================================================
```{r, eval=TRUE, echo=FALSE}
plot(emotionPlot)
```

How? (simplified version)
========================================================
- 3 categories: Positive, Negative, Neutral
- Database of tweets that have already been classified into those categories
- Tweet : "I love good food" 
- Split by words

How? (simplified version)
========================================================
- Food is found on 250 times in positive tweets
- Number of words on positive tweets: 1000
- P(Food|Positive) = 250/ 1000 = 25%
- I Love good food
- 25% * 62% * 74% * 25% = 0.03
- That's the Probability of the tweet being positive.
- Do the same for negative and neutral. Choose the category with the highest probability.


Making this real
========================================================
- Lots of data (too much for R)
- Stored tweets in hadoop
  - consumed thorugh kafkka
  - processed by spark 

Questions?  
========================================================
