<style>
.small-code pre code {
  font-size: 1em;
}

.midcenter {
    position: fixed;
    top: 50%;
    left: 50%;
}

.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}

</style>

Intro to extracting and analyzing twitter data
========================================================
author: Rafael Monge
date:   26-11-2015

```{r, echo=FALSE}
opts_chunk$set(out.width='750px', dpi=200)
library(twitteR)
library(RCurl)
library(base64enc)
library(httr)
library(tm)
library(ggplot2)
library(SnowballC)
library(reshape2)
library(sentiment)
library(cluster) 
library(dplyr)
library(tm)
source("GetTweets.R")
source("BuildCorpus.R")
source("keywordChart.R")
source("Clusters.R")
source("Sentiment.R")
```

Agenda
========================================================

- Requirements
- Twitter API
- Extracting Twitter data
- Building a corpus
- Clusters
- Sentiment Analysis
- Other considerations


Tools used
========================================================

- R
- RStudio
- R libraries:
	- twitteR
	- tm
	- cluster
	- sentiment
	 

Code and presentation
========================================================

https://github.com/rafamonge/Twitter

Accessing Twitter API
========================================================

1. Create a twitter account
2. Go to https://apps.twitter.com/app/new
3. Fill in the form sample values: Name = TwitterDemoForCOP, description = "test", website = "test.intel.com" and create the app.
4. Get/generate the access tokens.
5. You'll need several values for connecting to twitter.  
	-  Consumer Key (API Key)
	-  Consumer Secret (API Secret)
	-  Access Token
	-  Access Token Secret
	
Accessing Twitter API
========================================================

![Application settings](sourceImages/ApplicationSettings.png)

Accessing Twitter API
========================================================

![Access Tokens](sourceImages/AccessToken.png)

Twitter API
========================================================
type: section

REST API
========================================================
- Get access to tweets, trends, user information etc.
- Can filter tweets by
  - Keyword: "beer -root", "Intel"
  - longitude/latitude
  - dates
  - users
  - language
  - etc.
- https://dev.twitter.com/rest/reference/get/search/tweets

Streaming API
========================================================
- Get a constant stream of  tweets as they are created
- Can filter by:
  - simple keywords
  - location
  - users

Rate limiting
========================================================
- Twitter limits the number of requests 
- 450 requests  / 15 min window for REST
- 1% of tweets for Streaming API
- Can always pay for more tweets :)

Extracting Tweets using R  
========================================================
type: section

using twitteR library

Extracting Tweets using R  
========================================================
class: small-code

```{r, eval=FALSE}
consumerKey <- "consumer!!"
consumerSecret <- "consumerSecret!"
accessToken <- "token!!"
accessSecret <- "secret!!"

setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessSecret)
intelTweets <- searchTwitter("Intel", n=15000, lang="en")
tweets <- twListToDF(intelTweets)
```


What have we got?
========================================================
class: small-code

```{r, echo =FALSE}
tweets <- GetTweetsFromFile()
```

16 columns:
```{r, echo =FALSE}
names(tweets)
```



What have we got?
========================================================


```{r}
(tweets[2:5,1]) ## get some tweets
```


Posts by User
========================================================

```{r, results='hide', echo=FALSE}
userCount <- tweets %>% group_by(screenName) %>%
  summarise(count = length(id)) %>%
  arrange (desc(count))
userCount <- head(userCount, 10)
ggplot(userCount, aes(screenName, count)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Who are they?
========================================================
![Access Tokens](sourceImages/NeoIntel.png)

Who are they?
========================================================
![Access Tokens](sourceImages/BestDeals.png)


Building a Corpus
========================================================
type: section

using tm library



Creating the corpus
========================================================
class: small-code

```{r, echo = FALSE, results='hide', cache=TRUE}
  # build a corpus
  corpus <- Corpus(VectorSource(tweets$text))
  corpus <-tm_map(corpus, function(x) iconv(x, to='UTF-8', sub='byte'))
  corpus <- tm_map(corpus, tolower) 
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  # "run", "runs" and "running" all become "run").
  corpus <- tm_map(corpus, stemDocument)
  stopWords <- c(stopwords('english'))
  corpus <- tm_map(corpus, removeWords, stopWords)
  corpus <- tm_map(corpus, PlainTextDocument)
```

- Collection of texts and metadata
- Different formats (e.g. word documents, tweets)
- Applies common operations regardless of format
- Used to clean tweets
  - Lower case
  - remove punctuation
  - remove stop words
    ```{r, echo = FALSE}
stopWords[1:5]
```  
  - stem words (Running, runs, run -> run)

Transformed Text
========================================================
class: small-code

```{r, echo=FALSE}
sampleTramsformedText <- sapply(corpus[2:3], as.character)
names(sampleTramsformedText) <- NULL

(tweets[2:3,1]) ## get some tweets
sampleTramsformedText
```

Terms Matrix
========================================================
class: small-code

```{r, cache=TRUE, results='hide', echo=FALSE}
termsMatrix <- TermDocumentMatrix(corpus)
commonTermsMatrix <- removeSparseTerms(termsMatrix, sparse=0.96)
commonTermsDf <- as.data.frame(inspect(commonTermsMatrix))
commonTermsDf <- commonTermsDf[!rownames(commonTermsDf) %in% c('intel'),]
```

```{r}
commonTermsDf[1:5, 1:4]
```

Clustering
========================================================
type: section

Hierarchical Cluster - code
========================================================
class: small-code


1. Each tweets starts on its own cluster/group
2. Calculate distance between tweets (Tweets with common word counds will be closer)
2. Find the 2 closest clusters and merge them into a single cluster
3. Compute distances (similarities) between the new cluster and each of the old clusters.
4. Repeat steps 2 and 3 until there's only one cluster.


```{r, results='hide', echo=TRUE, echo=FALSE}

commonTermsDf.scale <- scale(commonTermsDf)
d <- dist(commonTermsDf.scale, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
```
  
Hierarchical Cluster - plot
========================================================

```{r, results='hide', echo=FALSE}
plot(fit) # display dendogram?
rect.hclust(fit, k=5, border="red")
```


Kmeans Cluster
========================================================
1. Defined K initial points (centroids)
2. Calculate distance of all data points to the centroids and define which one they are closer to
3. Redefine the centroids as the mean of data points closest to them
4. Repeat until the centroids don't change

```{r, echo=TRUE}
kfit <- kmeans(d, 5)   
```

kmeans plot
========================================================
class: small-code

```{r, eval=FALSE}
kplot<- clusplot(as.matrix(d), kfit$cluster)  
plot(kplot)
```

sentiment analysis
========================================================
type: section


```{r, eval=TRUE, echo=FALSE, cache=TRUE}
transformedText<- sapply(corpus, as.character)
sentimentDf <- GetSentimentAnalysisDf(transformedText)
polarityPlot <- GetPolarityPlot(sentimentDf)
emotionPlot <- GetEmotionPlot(sentimentDf)

```

polarity plot
========================================================
```{r, eval=TRUE, echo=FALSE}
plot(polarityPlot)
```

emotion plot 
========================================================
```{r, eval=TRUE, echo=FALSE}
plot(emotionPlot)
```

How?
========================================================
- 3 categories: Positive, Negative, Neutral
- Database of tweets that have already been classified into those categories
- Tweet : "I love good food" 
- Split by words

How?
========================================================
- Food is found on 455 times in positive tweets
- Number of words on positive tweets: 1211
- P(Food|Positive) = 455/ 1211 = 37%
- I Love good food
- 25% * 62% * 74%  42% = 0.05

How?
========================================================
- 0.05 * P(Positive)
- 0.05 * 0.33 = 0.016
- That's the Probability of the tweet being positive.
- Do the same for negative and neutral. Choose the category with the highest probability.


Making this real
========================================================
- Lots of data (too much for R)
- Stored tweets in hadoop
  - consumed thorugh kafkka
  - processed by spark 

Questions?  
========================================================




